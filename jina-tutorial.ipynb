{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jina "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud-Native Neural Search Framework for Any Kind of Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo : https://master-jina-dleunji.endpoint.ainize.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcQZrntABHo0"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4SY5PtFro2T",
    "outputId": "75ce75b7-aac1-4c99-f5f1-1a6304374037"
   },
   "outputs": [],
   "source": [
    "!pip install jina==2.1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izSLx4LWr2Zh",
    "outputId": "6246942a-58b2-4724-a41c-d3f55ee33808"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yb1hnbxjr5d3",
    "outputId": "1f2fcfda-52b0-4994-cfd3-878530386808"
   },
   "outputs": [],
   "source": [
    "pip install yolov5==5.0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ahnQrarr8yN",
    "outputId": "3fb2893a-b853-4b0e-d218-7c92edcdb1fe"
   },
   "outputs": [],
   "source": [
    "pip install lmdb==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Asim9HTsC0R",
    "outputId": "0d44bb2e-0521-4da9-b125-fdfa5adb9c31"
   },
   "outputs": [],
   "source": [
    "pip install jina-commons@git+https://github.com/jina-ai/jina-commons.git#egg=jina-commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gS9TR-HpdPSe",
    "outputId": "75b34b0e-2b0a-40ad-803c-159063896c72"
   },
   "outputs": [],
   "source": [
    "# clean up\n",
    "! rm -rf workspace images query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdr0WZ2jBO5S"
   },
   "source": [
    "**Downloading and unzipping data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6edmcg9-CkZ",
    "outputId": "90076da3-7c19-416e-bd48-c3170da156dd"
   },
   "outputs": [],
   "source": [
    "! wget https://open-images.s3.eu-central-1.amazonaws.com/data.zip\n",
    "! unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1c9QgBTBWYy"
   },
   "source": [
    "## **Executors**\n",
    "In this section, we will start developing the necessary executors, for both query and index flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAH11pDzBnAU"
   },
   "source": [
    "### **CLIPImageEncoder**\n",
    "This encoder encodes an image into embeddings using the CLIP model. \n",
    "We want an executor that loads the CLIP model and encodes it during the query and index flows. \n",
    "\n",
    "Our executor should:\n",
    "* support both **GPU** and **CPU**: That's why we will provision the `device` parameter and use it when encoding.\n",
    "* be able to process documents in batches in order to use our resources effectively: To do so, we will use the parameter `batch_size`\n",
    "* be able to encode the full image during the query flow and encode only chunks during the index flow: This can be achieved with `traversal_paths` and method `DocumentArray.batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3PkqQvHLnFA"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from jina import DocumentArray, Executor, requests\n",
    "from jina.logging.logger import JinaLogger\n",
    "from transformers import CLIPFeatureExtractor, CLIPModel\n",
    "\n",
    "\n",
    "class CLIPImageEncoder(Executor):\n",
    "    \"\"\"Encode image into embeddings using the CLIP model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name_or_path: str = \"openai/clip-vit-base-patch32\",\n",
    "        device: str = \"cpu\",\n",
    "        batch_size: int = 32,\n",
    "        traversal_paths: Tuple = (\"r\",),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.traversal_paths = traversal_paths\n",
    "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
    "\n",
    "        self.device = device\n",
    "        self.preprocessor = CLIPFeatureExtractor.from_pretrained(\n",
    "            pretrained_model_name_or_path\n",
    "        )\n",
    "        self.model = CLIPModel.from_pretrained(self.pretrained_model_name_or_path)\n",
    "        self.model.to(self.device).eval()\n",
    "\n",
    "    @requests\n",
    "    def encode(self, docs: Optional[DocumentArray], parameters: dict, **kwargs):\n",
    "        if docs is None:\n",
    "            return\n",
    "\n",
    "        traversal_paths = parameters.get(\"traversal_paths\", self.traversal_paths)\n",
    "        batch_size = parameters.get(\"batch_size\", self.batch_size)\n",
    "        document_batches_generator = docs.batch(\n",
    "            traversal_paths=traversal_paths,\n",
    "            batch_size=batch_size,\n",
    "            require_attr=\"blob\",\n",
    "        )\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for batch_docs in document_batches_generator:\n",
    "                blob_batch = [d.blob for d in batch_docs]\n",
    "                tensor = self._generate_input_features(blob_batch)\n",
    "\n",
    "\n",
    "                embeddings = self.model.get_image_features(**tensor)\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "                for doc, embed in zip(batch_docs, embeddings):\n",
    "                    doc.embedding = embed\n",
    "\n",
    "    def _generate_input_features(self, images):\n",
    "        input_tokens = self.preprocessor(\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_tokens = {\n",
    "            k: v.to(torch.device(self.device)) for k, v in input_tokens.items()\n",
    "        }\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBdhRATrWOmd"
   },
   "source": [
    "### **YoloV5Segmenter**\n",
    "Since we want to retrieve small images in bigger images, the technique that we will havily rely on is segmenting. Basicly, we want to do object detection on the indexed images. This will generate bounding boxes around objects detected in side the images. The detected objects will be extracted and added as chunks to the original documents.\n",
    "BTW, guess what is the state-of-the-art object detection model ?\n",
    "Right, we will use YoloV5.\n",
    "\n",
    "\n",
    "Our **YoloV5Segmenter** should be able to load the `ultralytics/yolov5` model from Torch hub, otherwise, load a custom model. To achieve this, the executor accepts parameter `model_name_or_path` which will be used when loading. We will implement the method `load` which checks if the model exists in the the Torch Hub, otherwise, loads it as a custom model.\n",
    "\n",
    "For our use case, we will just rely on `yolov5s` (small version of `yolov5`). Of course, for better quality, you can choose a more complicated model or your custom model.\n",
    "\n",
    "Furtheremore, we want **YoloV5Segmenter** to support both **GPU** and **CPU** and it should be able to process in batches. Again, this is as simple as adding parameters `device` and `batch_size` and using them during segmenting.\n",
    "\n",
    "To perform segmenting, we will implement method `_segment_docs` which performs the following steps:\n",
    "1. For each batch (a batch consists of several images), use the model to get predictions for each image\n",
    "2. Each prediction of an image can contain several detections (because yolov5 will extract as much bounding boxes as possible, along with their confidence scores). We will filter out detections whose scores are below the `confidence_threshold` to keep good quality.\n",
    "\n",
    "Each detection is actually 2 points -top left (x1, y1) and bottom right (x2, y2)- a confidence score and a class. We want be using the class of the detection, but it can be useful in other search applications.\n",
    "\n",
    "3. With the detections that we have, we create crops (using the 2 points returned). Finally, we add these crops to image documents as chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8iT8BqZNyzL"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "from jina import Document, DocumentArray, Executor, requests\n",
    "from jina.logging.logger import JinaLogger\n",
    "from jina_commons.batching import get_docs_batch_generator\n",
    "\n",
    "\n",
    "class YoloV5Segmenter(Executor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str = 'yolov5s',\n",
    "        confidence_threshold: float = 0.3,\n",
    "        batch_size: int = 32,\n",
    "        device: str = 'cuda',\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if device != 'cpu' and not device.startswith('cuda'):\n",
    "            self.logger.error('Torch device not supported. Must be cpu or cuda!')\n",
    "            raise RuntimeError('Torch device not supported. Must be cpu or cuda!')\n",
    "        if device == 'cuda' and not torch.cuda.is_available():\n",
    "            self.logger.warning(\n",
    "                'You tried to use GPU but torch did not detect your'\n",
    "                'GPU correctly. Defaulting to CPU. Check your CUDA installation!'\n",
    "            )\n",
    "            device = 'cpu'\n",
    "        self.device = torch.device(device)\n",
    "        self.model = self._load(self.model_name_or_path)\n",
    "\n",
    "    @requests\n",
    "    def segment(\n",
    "        self, docs: Optional[DocumentArray] = None, parameters: Dict = {}, **kwargs\n",
    "    ):\n",
    "\n",
    "        if docs:\n",
    "            document_batches_generator = get_docs_batch_generator(\n",
    "                docs,\n",
    "                traversal_path=['r'],\n",
    "                batch_size=parameters.get('batch_size', self.batch_size),\n",
    "                needs_attr='blob',\n",
    "            )\n",
    "            self._segment_docs(document_batches_generator, parameters=parameters)\n",
    "\n",
    "    def _segment_docs(self, document_batches_generator: Iterable, parameters: Dict):\n",
    "        with torch.no_grad():\n",
    "            for document_batch in document_batches_generator:\n",
    "                images = [d.blob for d in document_batch]\n",
    "                predictions = self.model(\n",
    "                    images,\n",
    "                    size=640,\n",
    "                    augment=False,\n",
    "                ).pred\n",
    "\n",
    "                for doc, prediction in zip(document_batch, predictions):\n",
    "                    for det in prediction:\n",
    "                        x1, y1, x2, y2, conf, cls = det\n",
    "                        if conf < parameters.get(\n",
    "                            'confidence_threshold', self.confidence_threshold\n",
    "                        ):\n",
    "                            continue\n",
    "                        crop = doc.blob[int(y1) : int(y2), int(x1) : int(x2), :]\n",
    "                        doc.chunks.append(Document(blob=crop))\n",
    "\n",
    "    def _load(self, model_name_or_path):\n",
    "        if model_name_or_path in torch.hub.list('ultralytics/yolov5'):\n",
    "            return torch.hub.load(\n",
    "                'ultralytics/yolov5', model_name_or_path, device=self.device\n",
    "            )\n",
    "        else:\n",
    "            return torch.hub.load(\n",
    "                'ultralytics/yolov5', 'custom', model_name_or_path, device=self.device\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSi4hBTmV8mR"
   },
   "source": [
    "**Indexers**\n",
    "After developing the encoder, we will need 2 kinds of indexers: \n",
    "1. SimpleIndexer: This indexer will take care of storing chunks of images. It also supports vector similarity search which is important match small query images against segments of original images.\n",
    "\n",
    "2. LMDBStorage: LMDB is a simple memory-mapped transactional key-value store. It is convenient for this example because we can use it to store original indexed images so that we can retrieve them later. We will use it to create LMDBStorage which offers 2 functionnalities: indexing documents and retrieving documents by ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQG41eMxdVE8"
   },
   "source": [
    "**SimpleIndexer**\n",
    "To implement SimpleIndexer, we can leverage jina's `DocumentArrayMemmap`. You can read about this data type [here](https://docs.jina.ai/fundamentals/document/documentarraymemmap-api/).\n",
    "\n",
    "Our indexer will create an instance of `DocumentArrayMemmap` when it's initialized. We want to store indexed documents inside the workspace folder that's why we pass the `workspace` attribute of the executor to `DocumentArrayMemmap`.\n",
    "\n",
    "To index, we implement the method `index` which is bound to the index flow. It's as simple as extending the received docs to `DocumentArrayMemmap` instance.\n",
    "\n",
    "On the other hand, for search, we implement the method `search`. We bind it to the query flow using the decorator `@requests(on='/search')`.\n",
    "In jina, searching for query documents can be done by adding the results to the `matches` attribute of each query document. Since docs is a `DocumentArray` we can use method `match` to match query against the indexed documents.\n",
    "Read more about `match` [here](https://docs.jina.ai/fundamentals/document/documentarray-api/#matching-documentarray-to-another).\n",
    "There's another detail here. We already indexed documents before search, but we need to match query documents against chunks of the indexed images. Luckily, `DocumentArray.match` allows us to specify the traversal paths of right-hand-side parameter with parameter `traversal_rdarray`. Since we want to match the left side docs (query) against the chunks of the right side docs (indexed docs), we can specify that `traversal_rdarray=['c']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu2whtYOL7mI"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Dict, Optional\n",
    "import inspect\n",
    "\n",
    "from jina import DocumentArray, Executor, requests\n",
    "from jina.logging.logger import JinaLogger\n",
    "from jina.types.arrays.memmap import DocumentArrayMemmap\n",
    "\n",
    "\n",
    "class SimpleIndexer(Executor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self._storage = DocumentArrayMemmap(\n",
    "            self.workspace, key_length=kwargs.get('key_length', 64)\n",
    "        )\n",
    "\n",
    "    @requests(on='/index')\n",
    "    def index(\n",
    "        self,\n",
    "        docs: Optional['DocumentArray'] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if docs:\n",
    "            self._storage.extend(docs)\n",
    "\n",
    "    @requests(on='/search')\n",
    "    def search(\n",
    "        self,\n",
    "        docs: Optional['DocumentArray'] = None,\n",
    "        parameters: Optional[Dict] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if not docs:\n",
    "            return\n",
    "\n",
    "        docs.match(self._storage, traversal_rdarray=['c'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp4t_8ZPgDEW"
   },
   "source": [
    "**LMDBStorage**\n",
    "\n",
    "In order to implement the LMDBStorage, we need the following parts:\n",
    "\n",
    "**I. Handler**\n",
    "\n",
    "This will be a context manager that we will use when we access our LMDB database. We will create it as a standalone class.\n",
    "\n",
    "\n",
    "**II. LMDBStorage constructor**\n",
    "\n",
    "The constructor should initialize a few attributes:\n",
    "* the `map_size` of the database\n",
    "* the `default_traversal_paths`. Actually we need traversal paths because we will not be traversing documents in the same way during index and query flows. During index, we want to store the root documents. However, during query, we need to get the matches of documents by ID.\n",
    "* the index file: again, to keep things clean, we will store the index file inside the workspace folder. Therefore we can use the `workspace` attribute.\n",
    "\n",
    "\n",
    "**III. `LMDBStorage.index`**\n",
    "\n",
    "In order to index documents, we first start a transaction (so that our Storage executor is ACID-compliant). Then, we traverse them according to the `traversal_paths` (will be root during). Finally, each document is serialized to string and then added to the database (the key is the document ID)\n",
    "\n",
    "\n",
    "**IV. `LMDBStorage.search`**\n",
    "\n",
    "Unlike search in the SimpleIndexer, we only which to get the matched Documents by ID and return them. Actually, the matched documents will be empty and will only contain the IDs. The goal is to return full matched documents using IDs.\n",
    "To accomplish this, again, we start a transaction, traverse the matched documents, get each matched document by ID and use the results to fill our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRlx1YVbfiVc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import lmdb\n",
    "from jina import Document, DocumentArray, Executor, requests\n",
    "from jina_commons import get_logger\n",
    "from jina_commons.indexers.dump import export_dump_streaming, import_metas\n",
    "\n",
    "\n",
    "class _LMDBHandler:\n",
    "    def __init__(self, file, map_size):\n",
    "        # see https://lmdb.readthedocs.io/en/release/#environment-class for usage\n",
    "        self.file = file\n",
    "        self.map_size = map_size\n",
    "\n",
    "    @property\n",
    "    def env(self):\n",
    "        return self._env\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._env = lmdb.Environment(\n",
    "            self.file,\n",
    "            map_size=self.map_size,\n",
    "            subdir=False,\n",
    "            readonly=False,\n",
    "            metasync=True,\n",
    "            sync=True,\n",
    "            map_async=False,\n",
    "            mode=493,\n",
    "            create=True,\n",
    "            readahead=True,\n",
    "            writemap=False,\n",
    "            meminit=True,\n",
    "            max_readers=126,\n",
    "            max_dbs=0,  # means only one db\n",
    "            max_spare_txns=1,\n",
    "            lock=True,\n",
    "        )\n",
    "        return self._env\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if hasattr(self, '_env'):\n",
    "            self._env.close()\n",
    "\n",
    "\n",
    "class LMDBStorage(Executor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        map_size: int = 1048576000,  # in bytes, 1000 MB\n",
    "        default_traversal_paths: List[str] = ['r'],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.map_size = map_size\n",
    "        self.default_traversal_paths = default_traversal_paths\n",
    "        self.file = os.path.join(self.workspace, 'db.lmdb')\n",
    "        if not os.path.exists(self.workspace):\n",
    "            os.makedirs(self.workspace)\n",
    "\n",
    "    def _handler(self):\n",
    "        return _LMDBHandler(self.file, self.map_size)\n",
    "\n",
    "    @requests(on='/index')\n",
    "    def index(self, docs: DocumentArray, parameters: Dict, **kwargs):\n",
    "        traversal_paths = parameters.get(\n",
    "            'traversal_paths', self.default_traversal_paths\n",
    "        )\n",
    "        if docs is None:\n",
    "            return\n",
    "        with self._handler() as env:\n",
    "            with env.begin(write=True) as transaction:\n",
    "                for d in docs.traverse_flat(traversal_paths):\n",
    "                    transaction.put(d.id.encode(), d.SerializeToString())\n",
    "\n",
    "    @requests(on='/search')\n",
    "    def search(self, docs: DocumentArray, parameters: Dict, **kwargs):\n",
    "        traversal_paths = parameters.get(\n",
    "            'traversal_paths', self.default_traversal_paths\n",
    "        )\n",
    "        if docs is None:\n",
    "            return\n",
    "        docs_to_get = docs.traverse_flat(traversal_paths)\n",
    "        with self._handler() as env:\n",
    "            with env.begin(write=True) as transaction:\n",
    "                for i, d in enumerate(docs_to_get):\n",
    "                    id = d.id\n",
    "                    serialized_doc = Document(transaction.get(d.id.encode()))\n",
    "                    d.update(serialized_doc)\n",
    "                    d.id = id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KH6EdlQlGZq"
   },
   "source": [
    "**SimpleRanker**\n",
    "\n",
    "You might think why do we need a ranker at all ?\n",
    "Actually, a ranker is needed because we will matching small query images against chunks of parent documents. But how can we get back to parent documents (aka full images) given the chunks ? And what if 2 chunks belonging to the same parent are matched ?\n",
    "We can solve this by aggregating the similarity scores of chunks that belong to the same parent (using an aggregation method, in our case, will be the `min` value).\n",
    "So, for each query document, we perform the following:\n",
    "\n",
    "1. We create an empty collection of parent scores. This collection will store, for each parent, a list of scores of its chunk documents.\n",
    "2. For each match, since it's a originally a chunk document, we can retrieve its `parent_id`. And it's also a match document so we get its match score and add that value to the parent socres collection.\n",
    "3. After processing all matches, we need to aggregate the scores of each parent using the `min` metric.\n",
    "4. Finally, using the aggregated score values of parents, we can create a new list of matches (this time consisting of parents, not chunks). We also need to sort the matches list by aggregated scores.\n",
    "\n",
    "When query documents exit the SimpleRanker, they now have matches consisting of parent documents. However, parent documents just have IDs. That why, during the previous steps, we created LMDBStorage so that we can actually retrieve parent documents by IDs and fill them with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Amlx0_nL2uV"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "from jina import Document, DocumentArray, Executor, requests\n",
    "\n",
    "\n",
    "class SimpleRanker(Executor):\n",
    "    def __init__(\n",
    "        self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metric = 'cosine'\n",
    "\n",
    "    @requests(on='/search')\n",
    "    def rank(\n",
    "        self, docs: Optional[DocumentArray] = None, parameters: Dict = {}, **kwargs\n",
    "    ):\n",
    "        if docs is None:\n",
    "            return\n",
    "\n",
    "        for doc in docs:\n",
    "            parents_scores = defaultdict(list)\n",
    "            for m in DocumentArray([doc]).traverse_flat(['m']):\n",
    "                parents_scores[m.parent_id].append(m.scores[self.metric].value)\n",
    "\n",
    "            # Aggregate match scores for parent document and\n",
    "            # create doc's match based on parent document of matched chunks\n",
    "            new_matches = []\n",
    "            for match_parent_id, scores in parents_scores.items():\n",
    "                score = min(scores)\n",
    "\n",
    "                new_matches.append(\n",
    "                    Document(id=match_parent_id, scores={self.metric: score})\n",
    "                )\n",
    "\n",
    "            # Sort the matches\n",
    "            doc.matches = new_matches\n",
    "            doc.matches.sort(key=lambda d: d.scores[self.metric].value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjXL3OXKKIgL"
   },
   "source": [
    "### Enabling GPU\n",
    "Usually, indexing takes some time because YoloV5Segmenter and CLIPImageEncoder can be slow. However, you can speed up indexing by enabling GPU. To do so, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "* Navigate to Editâ†’Notebook Settings\n",
    "* Select GPU from the Hardware Accelerator drop-down\n",
    "* Change the device to 'cuda' in the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-d-YFJlJ9ST"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYbgiCezowH_"
   },
   "source": [
    "## Indexing\n",
    "Now, after creating executors, it's time to use them in order to build an index Flow and index our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUMN5kbXpKou"
   },
   "source": [
    "### Building the index Flow\n",
    "We create a Flow object and add executors one after the other with the right parameters:\n",
    "\n",
    "1. YoloV5Segmenter: We should also specify the device\n",
    "2. CLIPImageEncoder: It also receives the device parameter. And since we only encode the chunks, we specify  `'traversal_paths': ['c']`\n",
    "3. SimpleIndexer: We need to specify the workspace parameter\n",
    "4. LMDBStorage: We also need to specify the workspace parameter. Furtheremore, the executor can be in parallel to the other branch. We can achieve this using `needs='gateway'`. Finally, we set `default_traversal_paths` to `['r']`\n",
    "5. A final executor which just waits for both branchs.\n",
    "\n",
    "After building the index Flow, we can plot it to verify that we're using the correct architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "vA11u6EwNQQh",
    "outputId": "18eda8da-7919-4322-bbbb-e128961342e0"
   },
   "outputs": [],
   "source": [
    "from jina import Flow\n",
    "index_flow = Flow().add(uses=YoloV5Segmenter, name='segmenter', uses_with={'device': device}) \\\n",
    "  .add(uses=CLIPImageEncoder, name='encoder', uses_with={'device': device, 'traversal_paths': ['c']}) \\\n",
    "  .add(uses=SimpleIndexer, name='chunks_indexer', workspace='workspace') \\\n",
    "  .add(uses=LMDBStorage, name='root_indexer', workspace='workspace', needs='gateway', uses_with={'default_traversal_paths': ['r']}) \\\n",
    "  .add(name='wait_both', needs=['root_indexer', 'chunks_indexer'])\n",
    "index_flow.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNtexomTqqdg"
   },
   "source": [
    "Now it's time to index the dataset that we have downloaded. Actually, we will index images inside the `images` folder.\n",
    "This helper function will convert image files into Jina Documents and yield them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4jcfZwmPl7j"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from jina import Document\n",
    "\n",
    "def input_generator():\n",
    "    for filename in glob('images/*.jpg'):\n",
    "        doc = Document(uri=filename, tags={'filename': filename})\n",
    "        doc.convert_image_uri_to_blob()\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPVk3IJxq-AL"
   },
   "source": [
    "The final step in this section is to send the input documents to the index Flow. Note that indexing can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677,
     "referenced_widgets": [
      "efec3ff484c14b419525013dec4d6500",
      "62d8fabbcb634fb39495b19e26e658df",
      "f838772750d24815a1ae609b73048eee",
      "ca50603d14d743768aeddbb57bf34646",
      "049070d7f716425098aa2caa4ca1f284",
      "cd61ae785b5945abab454e638153ee53",
      "6929db807ae34608b0144e7e7d1e40db",
      "baad2d91e7a4460aaf40305270f55edb",
      "0fec9c46c6534d34a0a935f9d64ed591",
      "10593fda5099479b92f90c5574b20a82",
      "3f701c52f3324238b4d6e33a08a9b860",
      "6fa0a319456f4b08b0edab224e6619c5",
      "63ebe08c94fe4279bab5b17400f0b2d7",
      "b80f187177074dafadf17840ae5126fe",
      "1087bf4240e44a6192f3f30e7e483963",
      "8455437350dd426bb3f4523e57e49fce",
      "bb1258351abc4ec185c1215fd38a306d",
      "a3c24c367f5948f6b470783a6536e3fe",
      "73e3cbbcf4a04efbb01bbde95edf3b1f",
      "f4f34d0e95014aea8e0bd692a7e5eb5a",
      "81c2f9e169344f4c9646e14cfac69613",
      "9197c5b60cd34de3855cb9602ed8df10",
      "53cb1d8fa66f4e1ba840615317409b9c",
      "26432f2dd98a4cae9bc8f129f2a5c865"
     ]
    },
    "id": "dxxcAuxbRGqs",
    "outputId": "04a41afd-0420-4566-b20f-a4aa13ea1d92"
   },
   "outputs": [],
   "source": [
    "with index_flow:\n",
    "    input_docs = input_generator()\n",
    "    index_flow.post(on='/index', inputs=input_docs, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hewIBnqurEQA"
   },
   "source": [
    "## Searching:\n",
    "Now, let's build the search Flow and use it in order to find sample query images.\n",
    "\n",
    "Our Flow contains the following executors:\n",
    "\n",
    "1. CLIPImageEncoder: It receives the device parameter. This time, since we want to encode root query documents, we specify that `'traversal_paths': ['r']`\n",
    "2. SimpleIndexer: We need to specify the workspace parameter\n",
    "3. SimpleRanker\n",
    "4. LMDBStorage: First we specify the workspace parameter. Then we need to use a different traversal paths. This time we will be traversing matches: `'default_traversal_paths': ['m']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1ewwZfXRSDK"
   },
   "outputs": [],
   "source": [
    "from jina import Flow\n",
    "query_flow = Flow().add(uses=CLIPImageEncoder, name='encoder', uses_with={'device': device, 'traversal_paths': ['r']}) \\\n",
    "  .add(uses=SimpleIndexer, name='chunks_indexer', workspace='workspace') \\\n",
    "  .add(uses=SimpleRanker, name='ranker') \\\n",
    "  .add(uses=LMDBStorage, workspace='workspace', name='root_indexer', uses_with={'default_traversal_paths': ['m']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80AXRGIpsD6c"
   },
   "source": [
    "Let's plot our Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "lxvqtUxcbMbw",
    "outputId": "0efe1074-6984-4731-9148-5d659fbb1b93"
   },
   "outputs": [],
   "source": [
    "query_flow.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI3hlqP0sGyo"
   },
   "source": [
    "We create the following helper function in order to plot the result documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epm6N87fuJKY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_docs(docs):\n",
    "    for doc in docs[:3]:\n",
    "        plt.imshow(doc.blob)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdmdSwv7sOaO"
   },
   "source": [
    "Finally, we can start querying. We will use images inside the query folder.\n",
    "For each image, we will create a Jina Document. Then we send our documents to the query Flow and receive the response. \n",
    "\n",
    "For each query document, we can print the image and it's top 3 search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8b7RGWYUYeBx",
    "outputId": "0f4761a9-0e77-4e4c-c48c-8e111c5e79b1"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "with query_flow:\n",
    "    docs = [Document(uri=filename) for filename in glob.glob('query/*.jpg')]\n",
    "    for doc in docs:\n",
    "        doc.convert_image_uri_to_blob()\n",
    "    resp = query_flow.post('/search', docs, return_results=True)\n",
    "for doc in resp[0].docs:\n",
    "    print('query:')\n",
    "    plt.imshow(doc.blob)\n",
    "    plt.show()\n",
    "    print('results:')\n",
    "    show_docs(doc.matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright 2020-2021 Jina AI Limited.  All rights reserved.\n",
    "\n",
    "\n",
    "                                 Apache License\n",
    "                           Version 2.0, January 2004\n",
    "                        http://www.apache.org/licenses/\n",
    "\n",
    "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
    "\n",
    "   1. Definitions.\n",
    "\n",
    "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
    "      and distribution as defined by Sections 1 through 9 of this document.\n",
    "\n",
    "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
    "      the copyright owner that is granting the License.\n",
    "\n",
    "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
    "      other entities that control, are controlled by, or are under common\n",
    "      control with that entity. For the purposes of this definition,\n",
    "      \"control\" means (i) the power, direct or indirect, to cause the\n",
    "      direction or management of such entity, whether by contract or\n",
    "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
    "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
    "\n",
    "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
    "      exercising permissions granted by this License.\n",
    "\n",
    "      \"Source\" form shall mean the preferred form for making modifications,\n",
    "      including but not limited to software source code, documentation\n",
    "      source, and configuration files.\n",
    "\n",
    "      \"Object\" form shall mean any form resulting from mechanical\n",
    "      transformation or translation of a Source form, including but\n",
    "      not limited to compiled object code, generated documentation,\n",
    "      and conversions to other media types.\n",
    "\n",
    "      \"Work\" shall mean the work of authorship, whether in Source or\n",
    "      Object form, made available under the License, as indicated by a\n",
    "      copyright notice that is included in or attached to the work\n",
    "      (an example is provided in the Appendix below).\n",
    "\n",
    "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
    "      form, that is based on (or derived from) the Work and for which the\n",
    "      editorial revisions, annotations, elaborations, or other modifications\n",
    "      represent, as a whole, an original work of authorship. For the purposes\n",
    "      of this License, Derivative Works shall not include works that remain\n",
    "      separable from, or merely link (or bind by name) to the interfaces of,\n",
    "      the Work and Derivative Works thereof.\n",
    "\n",
    "      \"Contribution\" shall mean any work of authorship, including\n",
    "      the original version of the Work and any modifications or additions\n",
    "      to that Work or Derivative Works thereof, that is intentionally\n",
    "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
    "      or by an individual or Legal Entity authorized to submit on behalf of\n",
    "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
    "      means any form of electronic, verbal, or written communication sent\n",
    "      to the Licensor or its representatives, including but not limited to\n",
    "      communication on electronic mailing lists, source code control systems,\n",
    "      and issue tracking systems that are managed by, or on behalf of, the\n",
    "      Licensor for the purpose of discussing and improving the Work, but\n",
    "      excluding communication that is conspicuously marked or otherwise\n",
    "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
    "\n",
    "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
    "      on behalf of whom a Contribution has been received by Licensor and\n",
    "      subsequently incorporated within the Work.\n",
    "\n",
    "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
    "      this License, each Contributor hereby grants to You a perpetual,\n",
    "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
    "      copyright license to reproduce, prepare Derivative Works of,\n",
    "      publicly display, publicly perform, sublicense, and distribute the\n",
    "      Work and such Derivative Works in Source or Object form.\n",
    "\n",
    "   3. Grant of Patent License. Subject to the terms and conditions of\n",
    "      this License, each Contributor hereby grants to You a perpetual,\n",
    "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
    "      (except as stated in this section) patent license to make, have made,\n",
    "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
    "      where such license applies only to those patent claims licensable\n",
    "      by such Contributor that are necessarily infringed by their\n",
    "      Contribution(s) alone or by combination of their Contribution(s)\n",
    "      with the Work to which such Contribution(s) was submitted. If You\n",
    "      institute patent litigation against any entity (including a\n",
    "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
    "      or a Contribution incorporated within the Work constitutes direct\n",
    "      or contributory patent infringement, then any patent licenses\n",
    "      granted to You under this License for that Work shall terminate\n",
    "      as of the date such litigation is filed.\n",
    "\n",
    "   4. Redistribution. You may reproduce and distribute copies of the\n",
    "      Work or Derivative Works thereof in any medium, with or without\n",
    "      modifications, and in Source or Object form, provided that You\n",
    "      meet the following conditions:\n",
    "\n",
    "      (a) You must give any other recipients of the Work or\n",
    "          Derivative Works a copy of this License; and\n",
    "\n",
    "      (b) You must cause any modified files to carry prominent notices\n",
    "          stating that You changed the files; and\n",
    "\n",
    "      (c) You must retain, in the Source form of any Derivative Works\n",
    "          that You distribute, all copyright, patent, trademark, and\n",
    "          attribution notices from the Source form of the Work,\n",
    "          excluding those notices that do not pertain to any part of\n",
    "          the Derivative Works; and\n",
    "\n",
    "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
    "          distribution, then any Derivative Works that You distribute must\n",
    "          include a readable copy of the attribution notices contained\n",
    "          within such NOTICE file, excluding those notices that do not\n",
    "          pertain to any part of the Derivative Works, in at least one\n",
    "          of the following places: within a NOTICE text file distributed\n",
    "          as part of the Derivative Works; within the Source form or\n",
    "          documentation, if provided along with the Derivative Works; or,\n",
    "          within a display generated by the Derivative Works, if and\n",
    "          wherever such third-party notices normally appear. The contents\n",
    "          of the NOTICE file are for informational purposes only and\n",
    "          do not modify the License. You may add Your own attribution\n",
    "          notices within Derivative Works that You distribute, alongside\n",
    "          or as an addendum to the NOTICE text from the Work, provided\n",
    "          that such additional attribution notices cannot be construed\n",
    "          as modifying the License.\n",
    "\n",
    "      You may add Your own copyright statement to Your modifications and\n",
    "      may provide additional or different license terms and conditions\n",
    "      for use, reproduction, or distribution of Your modifications, or\n",
    "      for any such Derivative Works as a whole, provided Your use,\n",
    "      reproduction, and distribution of the Work otherwise complies with\n",
    "      the conditions stated in this License.\n",
    "\n",
    "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
    "      any Contribution intentionally submitted for inclusion in the Work\n",
    "      by You to the Licensor shall be under the terms and conditions of\n",
    "      this License, without any additional terms or conditions.\n",
    "      Notwithstanding the above, nothing herein shall supersede or modify\n",
    "      the terms of any separate license agreement you may have executed\n",
    "      with Licensor regarding such Contributions.\n",
    "\n",
    "   6. Trademarks. This License does not grant permission to use the trade\n",
    "      names, trademarks, service marks, or product names of the Licensor,\n",
    "      except as required for reasonable and customary use in describing the\n",
    "      origin of the Work and reproducing the content of the NOTICE file.\n",
    "\n",
    "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
    "      agreed to in writing, Licensor provides the Work (and each\n",
    "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
    "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "      implied, including, without limitation, any warranties or conditions\n",
    "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
    "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
    "      appropriateness of using or redistributing the Work and assume any\n",
    "      risks associated with Your exercise of permissions under this License.\n",
    "\n",
    "   8. Limitation of Liability. In no event and under no legal theory,\n",
    "      whether in tort (including negligence), contract, or otherwise,\n",
    "      unless required by applicable law (such as deliberate and grossly\n",
    "      negligent acts) or agreed to in writing, shall any Contributor be\n",
    "      liable to You for damages, including any direct, indirect, special,\n",
    "      incidental, or consequential damages of any character arising as a\n",
    "      result of this License or out of the use or inability to use the\n",
    "      Work (including but not limited to damages for loss of goodwill,\n",
    "      work stoppage, computer failure or malfunction, or any and all\n",
    "      other commercial damages or losses), even if such Contributor\n",
    "      has been advised of the possibility of such damages.\n",
    "\n",
    "   9. Accepting Warranty or Additional Liability. While redistributing\n",
    "      the Work or Derivative Works thereof, You may choose to offer,\n",
    "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
    "      or other liability obligations and/or rights consistent with this\n",
    "      License. However, in accepting such obligations, You may act only\n",
    "      on Your own behalf and on Your sole responsibility, not on behalf\n",
    "      of any other Contributor, and only if You agree to indemnify,\n",
    "      defend, and hold each Contributor harmless for any liability\n",
    "      incurred by, or claims asserted against, such Contributor by reason\n",
    "      of your accepting any such warranty or additional liability.\n",
    "\n",
    "   END OF TERMS AND CONDITIONS\n",
    "\n",
    "   Copyright 2020-2021 Jina AI Limited\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "[Jina docs - Find Small Images Inside Large Images](https://docs.jina.ai/datatype/image/small-images-inside-big-images/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation-image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "049070d7f716425098aa2caa4ca1f284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0fec9c46c6534d34a0a935f9d64ed591": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f701c52f3324238b4d6e33a08a9b860",
       "IPY_MODEL_6fa0a319456f4b08b0edab224e6619c5"
      ],
      "layout": "IPY_MODEL_10593fda5099479b92f90c5574b20a82"
     }
    },
    "10593fda5099479b92f90c5574b20a82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1087bf4240e44a6192f3f30e7e483963": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26432f2dd98a4cae9bc8f129f2a5c865": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f701c52f3324238b4d6e33a08a9b860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b80f187177074dafadf17840ae5126fe",
      "max": 605247071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63ebe08c94fe4279bab5b17400f0b2d7",
      "value": 605247071
     }
    },
    "53cb1d8fa66f4e1ba840615317409b9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62d8fabbcb634fb39495b19e26e658df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63ebe08c94fe4279bab5b17400f0b2d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6929db807ae34608b0144e7e7d1e40db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fa0a319456f4b08b0edab224e6619c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8455437350dd426bb3f4523e57e49fce",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1087bf4240e44a6192f3f30e7e483963",
      "value": " 605M/605M [00:37&lt;00:00, 16.2MB/s]"
     }
    },
    "73e3cbbcf4a04efbb01bbde95edf3b1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9197c5b60cd34de3855cb9602ed8df10",
      "max": 14698491,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_81c2f9e169344f4c9646e14cfac69613",
      "value": 14698491
     }
    },
    "81c2f9e169344f4c9646e14cfac69613": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8455437350dd426bb3f4523e57e49fce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9197c5b60cd34de3855cb9602ed8df10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3c24c367f5948f6b470783a6536e3fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b80f187177074dafadf17840ae5126fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "baad2d91e7a4460aaf40305270f55edb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb1258351abc4ec185c1215fd38a306d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73e3cbbcf4a04efbb01bbde95edf3b1f",
       "IPY_MODEL_f4f34d0e95014aea8e0bd692a7e5eb5a"
      ],
      "layout": "IPY_MODEL_a3c24c367f5948f6b470783a6536e3fe"
     }
    },
    "ca50603d14d743768aeddbb57bf34646": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baad2d91e7a4460aaf40305270f55edb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6929db807ae34608b0144e7e7d1e40db",
      "value": " 3.98k/3.98k [00:00&lt;00:00, 13.0kB/s]"
     }
    },
    "cd61ae785b5945abab454e638153ee53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efec3ff484c14b419525013dec4d6500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f838772750d24815a1ae609b73048eee",
       "IPY_MODEL_ca50603d14d743768aeddbb57bf34646"
      ],
      "layout": "IPY_MODEL_62d8fabbcb634fb39495b19e26e658df"
     }
    },
    "f4f34d0e95014aea8e0bd692a7e5eb5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26432f2dd98a4cae9bc8f129f2a5c865",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_53cb1d8fa66f4e1ba840615317409b9c",
      "value": " 14.0M/14.0M [00:21&lt;00:00, 673kB/s]"
     }
    },
    "f838772750d24815a1ae609b73048eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd61ae785b5945abab454e638153ee53",
      "max": 3984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_049070d7f716425098aa2caa4ca1f284",
      "value": 3984
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
